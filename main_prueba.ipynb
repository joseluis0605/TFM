{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:24.318625Z",
     "start_time": "2025-09-06T12:26:24.311156Z"
    }
   },
   "source": [
    "# prueba del transformer\n",
    "import numpy as np\n",
    "import torch  # libreria principal de python\n",
    "import torch.nn as nn  # modulo para las redes neuronales\n",
    "import torch.optim as optim  # modulo para algoritmos de optimizacion en redes neuronales\n",
    "import torch.utils.data as data  # modulo para tratar con los datasets\n",
    "import math  # operaciones matematicas\n",
    "import copy  # para copiar objetos y estructuras\n",
    "\n",
    "import pandas as pd\n",
    "from Tokenizador.Tokenizador import TokenizadorBatch"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:24.421437Z",
     "start_time": "2025-09-06T12:26:24.327909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = pd.read_csv(\"dataset_train/dataset_English-Spanish_Translation_Dataset.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "dataset.head()"
   ],
   "id": "b4953a2107c032c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  english  spanish\n",
       "0     Go.      Ve.\n",
       "1     Go.    Vete.\n",
       "2     Go.    Vaya.\n",
       "3     Go.  Váyase.\n",
       "4     Hi.    Hola."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Al tratarse de un dataset donde le voy a pasar tanto ingles como español, voy a crearme dos datasets, uno para cada idioma. Luego los unire",
   "id": "7629f789e7c275ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:24.476846Z",
     "start_time": "2025-09-06T12:26:24.473985Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.tail()",
   "id": "3985ba464bd9b91b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  english  \\\n",
       "118959  There are four main causes of alcohol-related ...   \n",
       "118960  There are mothers and fathers who will lie awa...   \n",
       "118961  A carbon footprint is the amount of carbon dio...   \n",
       "118962  Since there are usually multiple websites on a...   \n",
       "118963  If you want to sound like a native speaker, yo...   \n",
       "\n",
       "                                                  spanish  \n",
       "118959  Hay cuatro causas principales de muertes relac...  \n",
       "118960  Hay madres y padres que se quedan despiertos d...  \n",
       "118961  Una huella de carbono es la cantidad de contam...  \n",
       "118962  Como suele haber varias páginas web sobre cual...  \n",
       "118963  Si quieres sonar como un hablante nativo, debe...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118959</th>\n",
       "      <td>There are four main causes of alcohol-related ...</td>\n",
       "      <td>Hay cuatro causas principales de muertes relac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118960</th>\n",
       "      <td>There are mothers and fathers who will lie awa...</td>\n",
       "      <td>Hay madres y padres que se quedan despiertos d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118961</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Una huella de carbono es la cantidad de contam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118962</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Como suele haber varias páginas web sobre cual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118963</th>\n",
       "      <td>If you want to sound like a native speaker, yo...</td>\n",
       "      <td>Si quieres sonar como un hablante nativo, debe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:24.517359Z",
     "start_time": "2025-09-06T12:26:24.514006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_reducido = dataset.head(1500)\n",
    "dataset_reducido"
   ],
   "id": "84126c2e2c66dd1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          english           spanish\n",
       "0             Go.               Ve.\n",
       "1             Go.             Vete.\n",
       "2             Go.             Vaya.\n",
       "3             Go.           Váyase.\n",
       "4             Hi.             Hola.\n",
       "...           ...               ...\n",
       "1495  We had fun.  Lo pasamos bien.\n",
       "1496  We laughed.       Nos reímos.\n",
       "1497  We like it.        Nos gusta.\n",
       "1498  We love it.      Nos encanta.\n",
       "1499  We made it.       Lo hicimos.\n",
       "\n",
       "[1500 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>We had fun.</td>\n",
       "      <td>Lo pasamos bien.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>We laughed.</td>\n",
       "      <td>Nos reímos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>We like it.</td>\n",
       "      <td>Nos gusta.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>We love it.</td>\n",
       "      <td>Nos encanta.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>We made it.</td>\n",
       "      <td>Lo hicimos.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:31.698151Z",
     "start_time": "2025-09-06T12:26:24.552587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Suponiendo que tu DataFrame se llama x_train_Ingles_Espanol y la columna se llama \"entrada\"\n",
    "tokenizer_batch = TokenizadorBatch(max_length=128, batch_size=1024)\n",
    "\n",
    "# Tokenizamos todo el DataFrame en batches\n",
    "x_train_tokenizado = tokenizer_batch.tokenizar_dataframe(dataset_reducido, \"english\")"
   ],
   "id": "f04a86b6c111cfd4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseluismezquitajimenez/PycharmProjects/TFM/TFM/Tokenizador/prueba.py:15: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  self.device = device if device else torch.device(\"mps\") if torch.has_mps else torch.device(\"cpu\")\n",
      "/Users/joseluismezquitajimenez/PycharmProjects/TFM/TFM/Tokenizador/prueba.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"entrada_encoder\"] = tokens_encoder\n",
      "/Users/joseluismezquitajimenez/PycharmProjects/TFM/TFM/Tokenizador/prueba.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"entrada_decoder\"] = tokens_decoder\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:31.762504Z",
     "start_time": "2025-09-06T12:26:31.760109Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.shape",
   "id": "806912a68534ef1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118964, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:31.775143Z",
     "start_time": "2025-09-06T12:26:31.772021Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.info",
   "id": "46a22dc2e4cea904",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                   english  \\\n",
       "0                                                     Go.   \n",
       "1                                                     Go.   \n",
       "2                                                     Go.   \n",
       "3                                                     Go.   \n",
       "4                                                     Hi.   \n",
       "...                                                   ...   \n",
       "118959  There are four main causes of alcohol-related ...   \n",
       "118960  There are mothers and fathers who will lie awa...   \n",
       "118961  A carbon footprint is the amount of carbon dio...   \n",
       "118962  Since there are usually multiple websites on a...   \n",
       "118963  If you want to sound like a native speaker, yo...   \n",
       "\n",
       "                                                  spanish  \n",
       "0                                                     Ve.  \n",
       "1                                                   Vete.  \n",
       "2                                                   Vaya.  \n",
       "3                                                 Váyase.  \n",
       "4                                                   Hola.  \n",
       "...                                                   ...  \n",
       "118959  Hay cuatro causas principales de muertes relac...  \n",
       "118960  Hay madres y padres que se quedan despiertos d...  \n",
       "118961  Una huella de carbono es la cantidad de contam...  \n",
       "118962  Como suele haber varias páginas web sobre cual...  \n",
       "118963  Si quieres sonar como un hablante nativo, debe...  \n",
       "\n",
       "[118964 rows x 2 columns]>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:31.937825Z",
     "start_time": "2025-09-06T12:26:31.811346Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_reducido.tail(20)",
   "id": "49eb8885a0636841",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          english                  spanish  \\\n",
       "1480  Was it fun?          ¿Fue divertido?   \n",
       "1481  Was it fun?          ¿Era divertido?   \n",
       "1482  Watch this.               Mira esto.   \n",
       "1483  Watch this.               Mirá esto.   \n",
       "1484  Watch this.              Mirad esto.   \n",
       "1485  Watch this.               Mire esto.   \n",
       "1486  Watch this.              Miren esto.   \n",
       "1487  We are men.           Somos hombres.   \n",
       "1488  We buy CDs.         Compramos cedés.   \n",
       "1489  We can pay.  Nosotros podemos pagar.   \n",
       "1490  We can try.      Podemos intentarlo.   \n",
       "1491  We can try.        Podemos probarlo.   \n",
       "1492  We can try.          Podemos probar.   \n",
       "1493  We can win.           Podemos ganar.   \n",
       "1494  We had fun.          Nos divertimos.   \n",
       "1495  We had fun.         Lo pasamos bien.   \n",
       "1496  We laughed.              Nos reímos.   \n",
       "1497  We like it.               Nos gusta.   \n",
       "1498  We love it.             Nos encanta.   \n",
       "1499  We made it.              Lo hicimos.   \n",
       "\n",
       "                                        entrada_encoder  \\\n",
       "1480  [[tensor(5282, device='mps:0'), tensor(442, de...   \n",
       "1481  [[tensor(5282, device='mps:0'), tensor(442, de...   \n",
       "1482  [[tensor(20413, device='mps:0'), tensor(903, d...   \n",
       "1483  [[tensor(20413, device='mps:0'), tensor(903, d...   \n",
       "1484  [[tensor(20413, device='mps:0'), tensor(903, d...   \n",
       "1485  [[tensor(20413, device='mps:0'), tensor(903, d...   \n",
       "1486  [[tensor(20413, device='mps:0'), tensor(903, d...   \n",
       "1487  [[tensor(1401, device='mps:0'), tensor(621, de...   \n",
       "1488  [[tensor(1401, device='mps:0'), tensor(22113, ...   \n",
       "1489  [[tensor(1401, device='mps:0'), tensor(831, de...   \n",
       "1490  [[tensor(1401, device='mps:0'), tensor(831, de...   \n",
       "1491  [[tensor(1401, device='mps:0'), tensor(831, de...   \n",
       "1492  [[tensor(1401, device='mps:0'), tensor(831, de...   \n",
       "1493  [[tensor(1401, device='mps:0'), tensor(831, de...   \n",
       "1494  [[tensor(1401, device='mps:0'), tensor(1902, d...   \n",
       "1495  [[tensor(1401, device='mps:0'), tensor(1902, d...   \n",
       "1496  [[tensor(1401, device='mps:0'), tensor(94518, ...   \n",
       "1497  [[tensor(1401, device='mps:0'), tensor(1884, d...   \n",
       "1498  [[tensor(1401, device='mps:0'), tensor(5161, d...   \n",
       "1499  [[tensor(1401, device='mps:0'), tensor(7228, d...   \n",
       "\n",
       "                                        entrada_decoder  \n",
       "1480  [[tensor(250004, device='mps:0'), tensor(5282,...  \n",
       "1481  [[tensor(250004, device='mps:0'), tensor(5282,...  \n",
       "1482  [[tensor(250004, device='mps:0'), tensor(20413...  \n",
       "1483  [[tensor(250004, device='mps:0'), tensor(20413...  \n",
       "1484  [[tensor(250004, device='mps:0'), tensor(20413...  \n",
       "1485  [[tensor(250004, device='mps:0'), tensor(20413...  \n",
       "1486  [[tensor(250004, device='mps:0'), tensor(20413...  \n",
       "1487  [[tensor(250004, device='mps:0'), tensor(1401,...  \n",
       "1488  [[tensor(250004, device='mps:0'), tensor(1401,...  \n",
       "1489  [[tensor(250004, device='mps:0'), tensor(1401,...  \n",
       "1490  [[tensor(250004, device='mps:0'), tensor(1401,...  \n",
       "1491  [[tensor(250004, device='mps:0'), tensor(1401,...  \n",
       "1492  [[tensor(250004, device='mps:0'), tensor(1401,...  \n",
       "1493  [[tensor(250004, device='mps:0'), tensor(1401,...  \n",
       "1494  [[tensor(250004, device='mps:0'), tensor(1401,...  \n",
       "1495  [[tensor(250004, device='mps:0'), tensor(1401,...  \n",
       "1496  [[tensor(250004, device='mps:0'), tensor(1401,...  \n",
       "1497  [[tensor(250004, device='mps:0'), tensor(1401,...  \n",
       "1498  [[tensor(250004, device='mps:0'), tensor(1401,...  \n",
       "1499  [[tensor(250004, device='mps:0'), tensor(1401,...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "      <th>entrada_encoder</th>\n",
       "      <th>entrada_decoder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>Was it fun?</td>\n",
       "      <td>¿Fue divertido?</td>\n",
       "      <td>[[tensor(5282, device='mps:0'), tensor(442, de...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(5282,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>Was it fun?</td>\n",
       "      <td>¿Era divertido?</td>\n",
       "      <td>[[tensor(5282, device='mps:0'), tensor(442, de...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(5282,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>Watch this.</td>\n",
       "      <td>Mira esto.</td>\n",
       "      <td>[[tensor(20413, device='mps:0'), tensor(903, d...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(20413...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>Watch this.</td>\n",
       "      <td>Mirá esto.</td>\n",
       "      <td>[[tensor(20413, device='mps:0'), tensor(903, d...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(20413...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>Watch this.</td>\n",
       "      <td>Mirad esto.</td>\n",
       "      <td>[[tensor(20413, device='mps:0'), tensor(903, d...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(20413...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>Watch this.</td>\n",
       "      <td>Mire esto.</td>\n",
       "      <td>[[tensor(20413, device='mps:0'), tensor(903, d...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(20413...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>Watch this.</td>\n",
       "      <td>Miren esto.</td>\n",
       "      <td>[[tensor(20413, device='mps:0'), tensor(903, d...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(20413...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>We are men.</td>\n",
       "      <td>Somos hombres.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(621, de...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>We buy CDs.</td>\n",
       "      <td>Compramos cedés.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(22113, ...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>We can pay.</td>\n",
       "      <td>Nosotros podemos pagar.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(831, de...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>We can try.</td>\n",
       "      <td>Podemos intentarlo.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(831, de...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>We can try.</td>\n",
       "      <td>Podemos probarlo.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(831, de...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>We can try.</td>\n",
       "      <td>Podemos probar.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(831, de...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>We can win.</td>\n",
       "      <td>Podemos ganar.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(831, de...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>We had fun.</td>\n",
       "      <td>Nos divertimos.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(1902, d...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>We had fun.</td>\n",
       "      <td>Lo pasamos bien.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(1902, d...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>We laughed.</td>\n",
       "      <td>Nos reímos.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(94518, ...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>We like it.</td>\n",
       "      <td>Nos gusta.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(1884, d...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>We love it.</td>\n",
       "      <td>Nos encanta.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(5161, d...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>We made it.</td>\n",
       "      <td>Lo hicimos.</td>\n",
       "      <td>[[tensor(1401, device='mps:0'), tensor(7228, d...</td>\n",
       "      <td>[[tensor(250004, device='mps:0'), tensor(1401,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:31.991565Z",
     "start_time": "2025-09-06T12:26:31.990065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Device\n",
    "device = torch.device(\"cpu\")"
   ],
   "id": "750656e4051d141b",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:32.086586Z",
     "start_time": "2025-09-06T12:26:32.084807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparámetros\n",
    "batch_size = 64           # batch más pequeño, más rápido en CPU\n",
    "num_epochs = 50\n",
    "max_seq_length = 128      # truncamos secuencias largas\n",
    "src_vocab_size = 250006\n",
    "tgt_vocab_size = 250006\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "dropout = 0.1"
   ],
   "id": "c7a938763ab7653a",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:34.609651Z",
     "start_time": "2025-09-06T12:26:32.090686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicializar modelo, loss y optimizador\n",
    "from TFM.Transformer_Bloque.Transformer import Transformer\n",
    "\n",
    "transformer = Transformer( src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)"
   ],
   "id": "e74e7f8446400674",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:34.649816Z",
     "start_time": "2025-09-06T12:26:34.646047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# Función para convertir a tensores con padding dinámico\n",
    "# -------------------------\n",
    "def preparar_batch(batch_encoder, batch_decoder):\n",
    "    # Encontramos la longitud máxima de este batch\n",
    "    max_len_enc = max(len(seq) for seq in batch_encoder)\n",
    "    max_len_dec = max(len(seq) for seq in batch_decoder)\n",
    "\n",
    "    # Padding dinámico\n",
    "    encoder_padded = [seq + [0]*(max_len_enc - len(seq)) for seq in batch_encoder]\n",
    "    decoder_padded = [seq + [0]*(max_len_dec - len(seq)) for seq in batch_decoder]\n",
    "\n",
    "    # Convertir a tensores\n",
    "    entrada_encoder = torch.tensor(encoder_padded, dtype=torch.long, device=device)\n",
    "    entrada_decoder = torch.tensor(decoder_padded, dtype=torch.long, device=device)\n",
    "    return entrada_encoder, entrada_decoder\n"
   ],
   "id": "c4e0dc69b829c0b4",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:34.696022Z",
     "start_time": "2025-09-06T12:26:34.693511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Crear batches desde dataset\n",
    "def generar_batches(encoder_list, decoder_list, batch_size):\n",
    "    for i in range(0, len(encoder_list), batch_size):\n",
    "        batch_enc = encoder_list[i:i+batch_size]\n",
    "        batch_dec = decoder_list[i:i+batch_size]\n",
    "        yield preparar_batch(batch_enc, batch_dec)\n"
   ],
   "id": "8a5b811739025e26",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:26:35.238319Z",
     "start_time": "2025-09-06T12:26:34.734873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preparar listas de IDs\n",
    "encoder_ids = [fila.squeeze(0).tolist()[:max_seq_length] for fila in dataset_reducido[\"entrada_encoder\"]]\n",
    "decoder_ids = [fila.squeeze(0).tolist()[:max_seq_length] for fila in dataset_reducido[\"entrada_decoder\"]]\n"
   ],
   "id": "2a612e2c9098e11c",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:28:59.596004Z",
     "start_time": "2025-09-06T12:27:16.166881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Entrenamiento\n",
    "transformer.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for src_batch, tgt_batch in generar_batches(encoder_ids, decoder_ids, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        output = transformer(src_batch, tgt_batch[:, :-1])\n",
    "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size),\n",
    "                         tgt_batch[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/num_batches:.4f}\")\n",
    "\n",
    "# Guardar modelo entrenado\n",
    "torch.save(transformer.state_dict(), \"transformer_cpu.pth\")\n",
    "print(\"Modelo guardado en 'transformer_cpu.pth'\")\n"
   ],
   "id": "1617040771d5b242",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 7.0263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[83], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output\u001B[38;5;241m.\u001B[39mcontiguous()\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, tgt_vocab_size),\n\u001B[1;32m     15\u001B[0m                  tgt_batch[:, \u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mcontiguous()\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     16\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 17\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     20\u001B[0m num_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TFM/lib/python3.10/site-packages/torch/optim/optimizer.py:516\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    511\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    512\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    513\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    514\u001B[0m             )\n\u001B[0;32m--> 516\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    517\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    519\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TFM/lib/python3.10/site-packages/torch/optim/optimizer.py:81\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     79\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     80\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 81\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     83\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TFM/lib/python3.10/site-packages/torch/optim/adam.py:247\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    235\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    237\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    238\u001B[0m         group,\n\u001B[1;32m    239\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    244\u001B[0m         state_steps,\n\u001B[1;32m    245\u001B[0m     )\n\u001B[0;32m--> 247\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    262\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    264\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    266\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdecoupled_weight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TFM/lib/python3.10/site-packages/torch/optim/optimizer.py:149\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TFM/lib/python3.10/site-packages/torch/optim/adam.py:949\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    946\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    947\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 949\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    950\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    951\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    952\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    953\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    954\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    955\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    956\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    957\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    958\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    959\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    960\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    961\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    962\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    963\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    964\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    965\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    966\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    967\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    968\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    969\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/TFM/lib/python3.10/site-packages/torch/optim/adam.py:464\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001B[0m\n\u001B[1;32m    462\u001B[0m         exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 464\u001B[0m     \u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m    466\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable \u001B[38;5;129;01mor\u001B[39;00m differentiable:\n\u001B[1;32m    467\u001B[0m     step \u001B[38;5;241m=\u001B[39m step_t\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T12:27:01.912684Z",
     "start_time": "2025-09-06T12:17:31.394974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ],
   "id": "d3f78e5c6a5c4a39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape entrada_encoder: torch.Size([1500, 100])\n",
      "Shape entrada_decoder: torch.Size([1500, 100])\n",
      "Primer encoder: tensor([2016,    5,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "Primer decoder: tensor([250004,   2016,      5,      2,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0])\n",
      "Máximo ID encoder: 227865\n",
      "Máximo ID decoder: 250005\n",
      "Cantidad de ceros al final del primer encoder: 98\n",
      "Cantidad de ceros al final del primer decoder: 96\n",
      "Primer batch de 5 encoders:\n",
      "tensor([[2016,    5,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [2016,    5,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [2016,    5,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [2016,    5,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [2673,    5,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]])\n"
     ]
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
