{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prueba del transformer\n",
    "import numpy as np\n",
    "import torch  # libreria principal de python\n",
    "import torch.nn as nn  # modulo para las redes neuronales\n",
    "import torch.optim as optim  # modulo para algoritmos de optimizacion en redes neuronales\n",
    "import torch.utils.data as data  # modulo para tratar con los datasets\n",
    "import math  # operaciones matematicas\n",
    "import copy  # para copiar objetos y estructuras\n",
    "\n",
    "import pandas as pd\n",
    "from Tokenizador.Tokenizador import TokenizadorBatch"
   ],
   "id": "29cbfa40ff35af79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# cargamos el modelo completo\n",
    "transformer = torch.load(\"transformer_modelo_completo.pth\", map_location=torch.device(\"cpu\"))\n",
    "transformer.eval()"
   ],
   "id": "714cde1a935e4101"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")  # o tu device\n",
    "#transformer.eval()  # modo evaluación\n",
    "\n",
    "# Inicializamos tu tokenizador\n",
    "tokenizador = TokenizadorBatch(max_length=64, batch_size=1, device=device)\n",
    "\n",
    "# Frase de prueba\n",
    "frase_test = \"Llamame temprano\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Tokenizar entrada para el encoder\n",
    "# ---------------------------\n",
    "entrada_encoder, _ = tokenizador.tokenizar_fila(frase_test, frase_test)\n",
    "entrada_encoder = entrada_encoder.squeeze(0).unsqueeze(0)  # [1, seq_len]\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Inicializar decoder con token de inicio\n",
    "# ---------------------------\n",
    "bos_token_id = tokenizador.modelo_tokenizador.bos_token_id\n",
    "eos_token_id = tokenizador.modelo_tokenizador.eos_token_id\n",
    "dec_input = torch.tensor([[bos_token_id]], device=device)  # [1, 1]\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Generación paso a paso\n",
    "# ---------------------------\n",
    "max_len_dec = 20\n",
    "salida_ids = []\n",
    "\n",
    "for _ in range(max_len_dec):\n",
    "    with torch.no_grad():\n",
    "        # Forward del transformer\n",
    "        output = transformer(entrada_encoder, dec_input)  # [batch, seq_len, vocab_size]\n",
    "        \n",
    "        # Tomar el último token predicho\n",
    "        next_token = output[:, -1, :].argmax(-1).item()\n",
    "        salida_ids.append(next_token)\n",
    "        \n",
    "        # Concatenar el token predicho al decoder\n",
    "        dec_input = torch.cat([dec_input, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "        \n",
    "        # Si predice EOS, terminamos\n",
    "        if next_token == eos_token_id:\n",
    "            break\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Decodificar los IDs a texto\n",
    "# ---------------------------\n",
    "prediccion = tokenizador.modelo_tokenizador.decode(salida_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Entrada:\", frase_test)\n",
    "print(\"Predicción:\", prediccion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
